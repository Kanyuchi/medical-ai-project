{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical AI - Dataset Download & Setup\n",
    "\n",
    "This notebook downloads all required datasets directly to your Google Drive using Kaggle API.\n",
    "\n",
    "**Important:** Run this ONCE to download all datasets. They will be stored in your Google Drive for reuse.\n",
    "\n",
    "**Estimated Download Time:** 30-60 minutes (depending on internet speed)\n",
    "\n",
    "**Required Storage:** ~40-50 GB in Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Mount Google Drive & Check Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.chdir('/content/drive/MyDrive')\n",
    "print(\"‚úì Google Drive mounted successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available storage\n",
    "import shutil\n",
    "\n",
    "total, used, free = shutil.disk_usage(\"/content/drive/MyDrive\")\n",
    "\n",
    "print(\"Google Drive Storage:\")\n",
    "print(f\"Total: {total // (2**30)} GB\")\n",
    "print(f\"Used: {used // (2**30)} GB\")\n",
    "print(f\"Free: {free // (2**30)} GB\")\n",
    "print()\n",
    "\n",
    "if free < 50 * (2**30):  # 50 GB\n",
    "    print(\"‚ö†Ô∏è WARNING: You may not have enough space for all datasets.\")\n",
    "    print(\"Recommended: At least 50 GB free space\")\n",
    "    print(\"Consider downloading datasets one at a time.\")\n",
    "else:\n",
    "    print(\"‚úì Sufficient storage available for all datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Setup Kaggle API\n",
    "\n",
    "You'll need your Kaggle API credentials. Get them from: https://www.kaggle.com/settings/account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Kaggle API\n",
    "!pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup Kaggle credentials\nimport os\nimport json\nfrom getpass import getpass\n\n# Create .kaggle directory\n!mkdir -p ~/.kaggle\n\n# Enter your Kaggle credentials securely\n# Get these from: https://www.kaggle.com/settings/account -> Create New API Token\nprint(\"Enter your Kaggle credentials:\")\nkaggle_username = input(\"Kaggle Username: \")\nkaggle_key = getpass(\"Kaggle API Key (hidden): \")\n\n# Create kaggle.json\nkaggle_credentials = {\n    \"username\": kaggle_username,\n    \"key\": kaggle_key\n}\n\nwith open('/root/.kaggle/kaggle.json', 'w') as f:\n    json.dump(kaggle_credentials, f)\n\n# Set permissions\n!chmod 600 ~/.kaggle/kaggle.json\n\nprint(\"‚úì Kaggle API configured successfully\")\nprint(\"\\nTesting Kaggle connection...\")\n!kaggle datasets list --max-size 100"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Project Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create project structure in Google Drive\n",
    "import os\n",
    "\n",
    "base_dir = '/content/drive/MyDrive/medical-ai-project'\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# Create data directories\n",
    "data_dirs = [\n",
    "    f'{base_dir}/chest-xray-classification/data',\n",
    "    f'{base_dir}/skin-lesion-detection/data',\n",
    "    f'{base_dir}/drug-discovery/data',\n",
    "]\n",
    "\n",
    "for dir_path in data_dirs:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    print(f\"‚úì Created: {dir_path}\")\n",
    "\n",
    "print(\"\\n‚úì All directories created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Download Chest X-Ray Dataset\n",
    "\n",
    "**Dataset:** COVID-19 Radiography Database\n",
    "\n",
    "**Size:** ~3-5 GB\n",
    "\n",
    "**Classes:** COVID, Lung Opacity, Normal, Viral Pneumonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(f'{base_dir}/chest-xray-classification/data')\n",
    "\n",
    "print(\"Downloading COVID-19 Radiography Database...\")\n",
    "print(\"This may take 10-15 minutes...\\n\")\n",
    "\n",
    "!kaggle datasets download -d tawsifurrahman/covid19-radiography-database\n",
    "\n",
    "print(\"\\n‚úì Download complete!\")\n",
    "print(\"\\nExtracting files...\")\n",
    "\n",
    "!unzip -q covid19-radiography-database.zip\n",
    "!rm covid19-radiography-database.zip\n",
    "\n",
    "print(\"‚úì Chest X-Ray dataset ready!\")\n",
    "!ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify chest X-ray data\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(f'{base_dir}/chest-xray-classification/data')\n",
    "print(\"Chest X-Ray Dataset Structure:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for item in data_path.iterdir():\n",
    "    if item.is_dir():\n",
    "        num_files = len(list(item.glob('*.png'))) + len(list(item.glob('*.jpg')))\n",
    "        print(f\"üìÅ {item.name}: {num_files} images\")\n",
    "\n",
    "print(\"\\n‚úì Chest X-Ray dataset verification complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Download Skin Lesion Dataset\n",
    "\n",
    "**Dataset:** HAM10000 (Human Against Machine with 10000 training images)\n",
    "\n",
    "**Size:** ~5-8 GB\n",
    "\n",
    "**Classes:** 7 types of skin lesions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(f'{base_dir}/skin-lesion-detection/data')\n",
    "\n",
    "print(\"Downloading HAM10000 Skin Lesion Dataset...\")\n",
    "print(\"This may take 15-20 minutes...\\n\")\n",
    "\n",
    "!kaggle datasets download -d kmader/skin-cancer-mnist-ham10000\n",
    "\n",
    "print(\"\\n‚úì Download complete!\")\n",
    "print(\"\\nExtracting files...\")\n",
    "\n",
    "!unzip -q skin-cancer-mnist-ham10000.zip\n",
    "!rm skin-cancer-mnist-ham10000.zip\n",
    "\n",
    "print(\"‚úì Skin Lesion dataset ready!\")\n",
    "!ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify skin lesion data\n",
    "import pandas as pd\n",
    "\n",
    "data_path = Path(f'{base_dir}/skin-lesion-detection/data')\n",
    "print(\"Skin Lesion Dataset Structure:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check for metadata CSV\n",
    "csv_files = list(data_path.glob('*.csv'))\n",
    "if csv_files:\n",
    "    df = pd.read_csv(csv_files[0])\n",
    "    print(f\"\\nüìä Metadata file: {csv_files[0].name}\")\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    if 'dx' in df.columns:\n",
    "        print(\"\\nClass distribution:\")\n",
    "        print(df['dx'].value_counts())\n",
    "\n",
    "# Count image files\n",
    "for item in data_path.iterdir():\n",
    "    if item.is_dir():\n",
    "        num_files = len(list(item.glob('*.jpg'))) + len(list(item.glob('*.png')))\n",
    "        print(f\"\\nüìÅ {item.name}: {num_files} images\")\n",
    "\n",
    "print(\"\\n‚úì Skin Lesion dataset verification complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Download Drug Discovery Dataset\n",
    "\n",
    "**Dataset:** ESOL (Aqueous Solubility) or QM9\n",
    "\n",
    "**Size:** ~500 MB - 2 GB\n",
    "\n",
    "**Type:** SMILES strings with molecular properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Download from Kaggle\n",
    "os.chdir(f'{base_dir}/drug-discovery/data')\n",
    "\n",
    "print(\"Downloading molecular dataset...\")\n",
    "\n",
    "# QM9 dataset\n",
    "!kaggle datasets download -d burakhmmtgl/qm9-dataset\n",
    "!unzip -q qm9-dataset.zip\n",
    "!rm qm9-dataset.zip\n",
    "\n",
    "print(\"‚úì QM9 dataset ready!\")\n",
    "!ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Download ESOL using DeepChem (alternative)\n",
    "# Uncomment if you prefer ESOL dataset\n",
    "\n",
    "# !pip install -q deepchem\n",
    "# import deepchem as dc\n",
    "\n",
    "# print(\"Downloading ESOL dataset via DeepChem...\")\n",
    "# tasks, datasets, transformers = dc.molnet.load_esol(featurizer='ECFP', splitter='random')\n",
    "# train_dataset, valid_dataset, test_dataset = datasets\n",
    "\n",
    "# # Save to CSV\n",
    "# import pandas as pd\n",
    "# train_df = pd.DataFrame({\n",
    "#     'smiles': train_dataset.ids,\n",
    "#     'solubility': train_dataset.y.flatten()\n",
    "# })\n",
    "# train_df.to_csv('esol_train.csv', index=False)\n",
    "\n",
    "# print(\"‚úì ESOL dataset ready!\")\n",
    "# print(f\"Training samples: {len(train_dataset)}\")\n",
    "# print(f\"Validation samples: {len(valid_dataset)}\")\n",
    "# print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify drug discovery data\n",
    "data_path = Path(f'{base_dir}/drug-discovery/data')\n",
    "print(\"Drug Discovery Dataset Structure:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "csv_files = list(data_path.glob('*.csv'))\n",
    "if csv_files:\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file, nrows=5)\n",
    "        print(f\"\\nüìä File: {csv_file.name}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        print(f\"Sample data:\")\n",
    "        print(df.head())\n",
    "\n",
    "print(\"\\n‚úì Drug Discovery dataset verification complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Final Verification & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def get_dir_size(path):\n",
    "    \"\"\"Calculate directory size in GB\"\"\"\n",
    "    total = 0\n",
    "    for entry in Path(path).rglob('*'):\n",
    "        if entry.is_file():\n",
    "            total += entry.stat().st_size\n",
    "    return total / (1024**3)  # Convert to GB\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"          DATASET DOWNLOAD SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "datasets = [\n",
    "    ('Chest X-Ray Classification', f'{base_dir}/chest-xray-classification/data'),\n",
    "    ('Skin Lesion Detection', f'{base_dir}/skin-lesion-detection/data'),\n",
    "    ('Drug Discovery', f'{base_dir}/drug-discovery/data'),\n",
    "]\n",
    "\n",
    "total_size = 0\n",
    "for name, path in datasets:\n",
    "    if os.path.exists(path):\n",
    "        size = get_dir_size(path)\n",
    "        total_size += size\n",
    "        print(f\"\\n‚úì {name}\")\n",
    "        print(f\"  Location: {path}\")\n",
    "        print(f\"  Size: {size:.2f} GB\")\n",
    "        \n",
    "        # Count files\n",
    "        num_files = sum(1 for _ in Path(path).rglob('*') if _.is_file())\n",
    "        print(f\"  Files: {num_files:,}\")\n",
    "    else:\n",
    "        print(f\"\\n‚úó {name} - NOT FOUND\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"TOTAL SIZE: {total_size:.2f} GB\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüéâ All datasets downloaded successfully!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Open chest_xray_classification.ipynb to start training\")\n",
    "print(\"2. Or open skin_lesion_detection.ipynb\")\n",
    "print(\"3. Or open drug_discovery.ipynb\")\n",
    "print(\"\\nüí° Datasets are saved in your Google Drive and will persist across sessions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Download Additional Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative chest X-ray dataset (Pneumonia)\n",
    "# Uncomment to download\n",
    "\n",
    "# os.chdir(f'{base_dir}/chest-xray-classification/data')\n",
    "# !kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
    "# !unzip -q chest-xray-pneumonia.zip -d pneumonia_dataset\n",
    "# !rm chest-xray-pneumonia.zip\n",
    "# print(\"‚úì Pneumonia dataset downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISIC 2019 Skin Lesion Dataset (larger, more comprehensive)\n",
    "# Uncomment to download (WARNING: ~25 GB)\n",
    "\n",
    "# os.chdir(f'{base_dir}/skin-lesion-detection/data')\n",
    "# !kaggle datasets download -d nodoubttome/skin-cancer9-classesisic\n",
    "# !unzip -q skin-cancer9-classesisic.zip -d isic2019\n",
    "# !rm skin-cancer9-classesisic.zip\n",
    "# print(\"‚úì ISIC 2019 dataset downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup & Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üí° IMPORTANT TIPS:\")\n",
    "print(\"\\n1. Datasets are now in your Google Drive - they won't disappear!\")\n",
    "print(\"2. You can disconnect from Colab and come back later\")\n",
    "print(\"3. Each training notebook will mount Drive and access these datasets\")\n",
    "print(\"4. No need to re-download unless you delete the data\")\n",
    "print(\"\\n5. Recommended training order:\")\n",
    "print(\"   a) Chest X-Ray (4-6 hours) - Start here\")\n",
    "print(\"   b) Skin Lesion (4-6 hours)\")\n",
    "print(\"   c) Drug Discovery (4-8 hours)\")\n",
    "print(\"\\n6. You can train multiple models across different sessions\")\n",
    "print(\"\\n7. Your Colab Pro+ compute units: 1131.86 units\")\n",
    "print(\"   This is MORE than enough for all three projects!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}